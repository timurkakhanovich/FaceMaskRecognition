{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.nn.modules.distance import PairwiseDistance\n",
    "from torch.utils.data import DataLoader \n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "from training_utils.LFWDataset import (\n",
    "    TripletTrainingDataset,\n",
    "    TriletValidatingDataset,\n",
    ")\n",
    "from training_utils.validation import evaluate_lfw\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "torch.cuda.empty_cache()\n",
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def validate_lfw(model, lfw_dataloader):\n",
    "    model.eval()\n",
    "    l2_distance = PairwiseDistance(p=2)\n",
    "    distances, labels = [], []\n",
    "\n",
    "    for data_a, data_b, label in tqdm(lfw_dataloader):\n",
    "        data_a = data_a.to(device)\n",
    "        data_b = data_b.to(device)\n",
    "\n",
    "        output_a, output_b = model(data_a), model(data_b)\n",
    "        distance = l2_distance.forward(output_a, output_b)\n",
    "\n",
    "        distances.append(distance.cpu().detach().numpy())\n",
    "        labels.append(label.cpu().detach().numpy())\n",
    "\n",
    "    labels = np.array([sublabel for label in labels for sublabel in label])\n",
    "    distances = np.array([subdist for distance in distances for subdist in distance])\n",
    "\n",
    "    _, _, precision, recall, accuracy, roc_auc, best_distances, TAR, FAR = \\\n",
    "        evaluate_lfw(\n",
    "            distances=distances,\n",
    "            labels=labels,\n",
    "            far_target=1e-1\n",
    "        )\n",
    "\n",
    "    accuracy = np.mean(accuracy)\n",
    "    precision = np.mean(precision)\n",
    "    recall = np.mean(recall)\n",
    "    f1 = 2*precision*recall/(precision + recall)\n",
    "    tar = np.mean(TAR)\n",
    "    far = np.mean(FAR)\n",
    "\n",
    "    best_threshold = np.mean(best_distances)\n",
    "    print(\n",
    "        f\"Accuracy on LFW: {accuracy}\\n\"\n",
    "        f\"Precision: {precision}\\n\"\n",
    "        f\"Recall: {recall}\\n\"\n",
    "        f\"F1-score: {f1}\\n\"\n",
    "        f\"ROC Area Under Curve: {roc_auc}\\n\"\n",
    "        f\"Best distance threshold: {best_threshold}\\n\"\n",
    "        f\"TAR: {tar} @ FAR: {far}\"\n",
    "    )\n",
    "\n",
    "    return best_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocess = {\n",
    "    \"train\": \n",
    "        transforms.Compose([\n",
    "        transforms.Resize(size=224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.6068, 0.4517, 0.3800],\n",
    "            std=[0.2492, 0.2173, 0.2082]\n",
    "        )\n",
    "    ]), \n",
    "    \"val\":\n",
    "        transforms.Compose([\n",
    "        transforms.Resize(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.6068, 0.4517, 0.3800],\n",
    "            std=[0.2492, 0.2173, 0.2082]\n",
    "        )\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = { \n",
    "    \"val\": TriletValidatingDataset(\"Data/val/\", \"lfw_pairs.txt\", transform=data_preprocess[\"val\"]),\n",
    "    \"test\": TriletValidatingDataset(\"Data/test\", \"lfw_pairs_test.txt\", transform=data_preprocess[\"val\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    \"val\": DataLoader(\n",
    "        dataset=datasets[\"val\"],\n",
    "        batch_size=32,\n",
    "        num_workers=0,\n",
    "        shuffle=False,\n",
    "    ),\n",
    "    \"test\": DataLoader(\n",
    "        dataset=datasets[\"test\"],\n",
    "        batch_size=32,\n",
    "        num_workers=0,\n",
    "        shuffle=False,\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"checkpoints/train_1/checkpoint_epoch_174.pt\")\n",
    "\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "best_distance_threshold = checkpoint[\"best_distance_threshold\"]\n",
    "curr_model_epoch = checkpoint[\"epoch\"]\n",
    "\n",
    "try:\n",
    "    prev_losses = checkpoint[\"losses\"]\n",
    "except KeyError:\n",
    "    prev_losses = []\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adagrad(model.parameters(), lr=1e-4, initial_accumulator_value=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_model_state_dict\"])\n",
    "LR_scheduler = lr_scheduler.StepLR(optimizer, step_size=15, gamma=1)  # for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    optimizer,\n",
    "    scheduler=None,\n",
    "    num_epochs=5,\n",
    "    start_epoch=-1,\n",
    "    margin=0.2, \n",
    "    hard_triplet=True,\n",
    "    prev_losses=[],\n",
    "):\n",
    "    epoch_dataset_size = 0\n",
    "    l2_distance = PairwiseDistance(p=2)\n",
    "    tripletloss = nn.TripletMarginLoss(margin=margin, p=2)\n",
    "    epoch_losses = prev_losses[:]\n",
    "\n",
    "    for epoch in range(start_epoch + 1, start_epoch + 1 + num_epochs):\n",
    "        running_corrects = 0.0\n",
    "        running_loss = 0.0\n",
    "        epoch_dataset_size = 0\n",
    "\n",
    "        print(f\"Epoch {epoch}/{start_epoch + num_epochs}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "        datasets = { \n",
    "            \"train\": TripletTrainingDataset(\n",
    "                root_dir=Path(\"Data/train\"),\n",
    "                batch_size=32,\n",
    "                num_triplets=6144,\n",
    "                transform=data_preprocess[\"train\"],\n",
    "            ),\n",
    "            \"val\": TriletValidatingDataset(\n",
    "                root_dir=Path(\"Data/val/\"),\n",
    "                pairs_path=Path(\"Datasets/lfw_pairs.txt\"),\n",
    "                transform=data_preprocess[\"val\"],\n",
    "            ),\n",
    "        }\n",
    "        \n",
    "        dataloaders = {\n",
    "            \"train\": DataLoader(datasets[\"train\"], shuffle=True),\n",
    "            \"val\": DataLoader(\n",
    "                dataset=datasets[\"val\"],\n",
    "                batch_size=32,\n",
    "                num_workers=0,\n",
    "                shuffle=False\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for data in tqdm(dataloaders[\"train\"]):\n",
    "            anch_inputs = torch.stack([d[\"anc_img\"] for d in data]).squeeze().cuda()\n",
    "            pos_inputs = torch.stack([d[\"pos_img\"] for d in data]).squeeze().cuda()\n",
    "            neg_inputs = torch.stack([d[\"neg_img\"] for d in data]).squeeze().cuda()\n",
    "\n",
    "            anch_outputs = model(anch_inputs)\n",
    "            pos_outputs = model(pos_inputs)\n",
    "            neg_outputs = model(neg_inputs)\n",
    "\n",
    "            pos_distance = l2_distance(anch_outputs, pos_outputs)\n",
    "            neg_distance = l2_distance(anch_outputs, neg_outputs)\n",
    "\n",
    "            if hard_triplet:\n",
    "                hard_triplets_correct = (neg_distance - pos_distance < margin).cpu().numpy().flatten()\n",
    "\n",
    "                triplets_indices = np.where(hard_triplets_correct == True)[0]\n",
    "\n",
    "            else:\n",
    "                first_cond = (neg_distance - pos_distance < margin).cpu().numpy().flatten()\n",
    "                second_cond = (pos_distance < neg_distance).cpu().numpy().flatten()\n",
    "\n",
    "                semihard_triplets_correct = np.logical_and(first_cond, second_cond)\n",
    "\n",
    "                triplets_indices = np.where(semihard_triplets_correct == True)[0]\n",
    "\n",
    "            anch_triplet = anch_outputs[triplets_indices]\n",
    "            pos_triplet = pos_outputs[triplets_indices]\n",
    "            neg_triplet = neg_outputs[triplets_indices]\n",
    "\n",
    "            loss = tripletloss(anch_triplet, pos_triplet, neg_triplet)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            if not np.isnan(loss.item()):    \n",
    "                running_loss += loss.item() * len(triplets_indices)\n",
    "            running_corrects += len(data) - len(triplets_indices)\n",
    "            epoch_dataset_size += len(data)\n",
    "    \n",
    "        epoch_loss = running_loss / len(dataloaders[\"train\"])\n",
    "        epoch_losses.append(epoch_loss)\n",
    "\n",
    "        # it's implied that source dataloaders[\"train\"] is taken from datasets[\"train\"] \n",
    "        epoch_acc = running_corrects / epoch_dataset_size\n",
    "\n",
    "        print(\"Train Loss: {:.4f} Acc: {:.4f}\".format(\n",
    "                epoch_loss, epoch_acc))\n",
    "\n",
    "        model.eval()\n",
    "        best_distances = validate_lfw(model, dataloaders[\"val\"])\n",
    "\n",
    "        state = {\n",
    "            \"epoch\": epoch,\n",
    "            \"embedding_dimension\": checkpoint[\"embedding_dimension\"],\n",
    "            \"batch_size_training\": len(dataloaders[\"train\"]),\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"model_architecture\": checkpoint[\"model_architecture\"],\n",
    "            \"optimizer_model_state_dict\": optimizer.state_dict(),\n",
    "            \"best_distance_threshold\": np.mean(best_distances),\n",
    "            \"losses\": epoch_losses\n",
    "        }\n",
    "        \n",
    "        del dataloaders, datasets\n",
    "        gc.collect()\n",
    "        \n",
    "        torch.save(state, f\"checkpoint_epoch_{epoch}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, optimizer, LR_scheduler, num_epochs=70, start_epoch=curr_model_epoch, margin=0.5, prev_losses=prev_losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "name": "python37464bitbaseconda4f12520a107c44ccaaacdf341cde0036"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "metadata": {
   "interpreter": {
    "hash": "3f800561dde6209f0c647b1ec24b295364b37801e2a63d392a491285ef4d5a88"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
